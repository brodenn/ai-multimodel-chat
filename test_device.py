from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "../deepseek-r1-qwen32b"

# Ladda tokenizer
print("ğŸ” Laddar tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# Ladda modell
print("ğŸ” Laddar modell...")
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Kontrollera vilken enhet modellen ligger pÃ¥
device_info = {}
for name, param in model.named_parameters():
    print(f"âœ… FÃ¶rsta parametern: {name} â†’ {param.device}")
    break  # Endast fÃ¶rsta parametern

# Extra: kontrollera om modellen alls anvÃ¤nder GPU
gpu_detected = any(param.device.type == "cuda" for param in model.parameters())

if gpu_detected:
    print("ğŸš€ Modellen Ã¤r laddad pÃ¥ GPU! âœ”ï¸")
else:
    print("âš ï¸ Modellen ligger fortfarande pÃ¥ CPU. âŒ")
    print("ğŸ’¡ Tips:")
    print(" - Kontrollera att ROCm fungerar med PyTorch (testa: torch.cuda.is_available())")
    print(" - AnvÃ¤nd istÃ¤llet .to('cuda') manuellt")
    print(" - Undvik device_map='auto' om den ignoreras av ROCm")

# BekrÃ¤fta att CUDA Ã¤r tillgÃ¤ngligt
print(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
